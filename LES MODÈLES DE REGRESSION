# Régression Linéaire multiple
import numpy as np
import pandas as pd 
import random 
import matplotlib.pyplot as plt 
%matplotlib inline
from matplotlib.pylab import rcParams
######################################
rcParams['figure.figsize']=12,10
#Define input array with angles from 60deg to 300deg converted to radians
x = np.array([i*np.pi/180 for i in range(60,300,4)])
#print(x)
np.random.seed(777)  #Jackpot
y = np.sin(x) + np.random.normal(0,0.15,len(x))
data = pd.DataFrame(np.column_stack([x,y]),columns=['x','y'])
print(data.head())
#faites plot de x Vs Y
plt.plot(data['x'],data['y'],'.', color='b')
#########################################
for i in range(2,16):  # power of 1 is already there
    colname = 'x_%d'%i      # new var will be x_power
    data[colname] = data['x']**i
#######################################
#Import Linear Regression model from scikit-learn.
# pip install sklearn 
from sklearn.linear_model import LinearRegression

def linear_regression(data, power, models_to_plot):
    #initialize predictors:
    predictors=['x']
    
    if power>=2:
        predictors.extend(['x_%d'%i for i in range(2,power+1)])
    
    #print(predictors)
    
    #Fit the model
    linreg = LinearRegression(normalize=True) #In theory, normalising the predictors 
                                             #will not affect the predictions made by linear regression. 
                                            #However, there are some practical reasons for doing so.
    
    linreg.fit(data[predictors],data['y']) # find optimal weights w
    
    y_pred = linreg.predict(data[predictors])  # compute  ΣwX
    
    
    #Check if a plot is to be made for the entered power
    if power in models_to_plot:
        plt.subplot(models_to_plot[power])
        plt.tight_layout()
        plt.plot(data['x'],y_pred, linewidth=4, color = 'r')
        plt.plot(data['x'],data['y'],'.',color = 'b')
        plt.title('Plot for power: %d'%power)
    
    #Return the result in pre-defined format
    rss = sum((y_pred-data['y'])**2)

    ret = [rss]
    #Reminder
    #x = [1, 2, 3] x.append([4, 5]) print (x) [1, 2, 3, [4, 5]]
    #x = [1, 2, 3] x.extend([4, 5]) print (x) [1, 2, 3, 4, 5]
    
    ret.extend([linreg.intercept_]) 
    ret.extend(linreg.coef_)
    return ret
    #################################£
    #Initialize a dataframe to store the results:
col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]
ind = ['model_pow_%d'%i for i in range(1,16)]
coef_matrix_simple = pd.DataFrame(index=ind, columns=col)
#print(coef_matrix_simple)
##########################
rcParams['figure.figsize'] = 12, 10

#Define the powers for which a plot is required:
models_to_plot = {1:231,3:232,6:233,9:234,12:235,15:236} #dictionnary where 1, 3, 6, 9, 12 and 15 are 
                                                        #the powers to be plot

#for power in models_to_plot:
#    print(power)
#    print(models_to_plot[power])



#Iterate through all powers and assimilate results
for i in range(1,16):
    coef_matrix_simple.iloc[i-1,0:i+2] = linear_regression(data, power=i, models_to_plot=models_to_plot)
#pour i=1
#ret=[rss,intecept, coef_1]
###################################
#Set the display format to be scientific for ease of analysis
#pd.options.display.float_format = '{:,.2g}'.format
coef_matrix_simple

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# RIDGE REGRESSION 
from sklearn.linear_model import Ridge
def ridge_regression(data, predictors, alpha, models_to_plot={}):
    #Fit the model
    ridgereg = Ridge(alpha=alpha,normalize=True)
    ridgereg.fit(data[predictors],data['y'])
    y_pred = ridgereg.predict(data[predictors])
    
    #Check if a plot is to be made for the entered alpha
    if alpha in models_to_plot:
        plt.subplot(models_to_plot[alpha])
        plt.tight_layout()
        plt.plot(data['x'],y_pred, linewidth=4, color = 'r')
        plt.plot(data['x'],data['y'],'.',color = 'b')
        plt.title('Plot for alpha: %.3g'%alpha)
    
    #Return the result in pre-defined format
    rss = sum((y_pred-data['y'])**2)
    ret = [rss]
    ret.extend([ridgereg.intercept_])
    ret.extend(ridgereg.coef_)
    return ret
    ######################
    #Initialize predictors to be set of 15 powers of x
predictors=['x']
predictors.extend(['x_%d'%i for i in range(2,16)])

#Set the different values of alpha to be tested
alpha_ridge = [1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]

#Initialize the dataframe for storing coefficients.
col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]
ind = ['alpha_%.2g'%alpha_ridge[i] for i in range(0,10)]
coef_matrix_ridge = pd.DataFrame(index=ind, columns=col)

models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}
for i in range(10):
    coef_matrix_ridge.iloc[i,] = ridge_regression(data, predictors, alpha_ridge[i], models_to_plot)
###############################
#Set the display format to be scientific for ease of analysis
#pd.options.display.float_format = '{:,.2g}'.format
coef_matrix_ridge
# determining the number of zeros in each row of the coefficients data set:
coef_matrix_ridge.apply(lambda x: sum(x.values==0),axis=1)
#%%%%%%%%%%%%%%%%%%%
# LASSO RÉGRESSION 
from sklearn.linear_model import Lasso
def lasso_regression(data, predictors, alpha, models_to_plot={}):
    #Fit the model
    lassoreg = Lasso(alpha=alpha,normalize=True, max_iter=1e5)
    lassoreg.fit(data[predictors],data['y'])
    y_lasso=lassoreg.predict(data[predictors])
    if alpha in models_to_plot:
        plt.subplot(models_to_plot[alpha])
        plt.tight_layout()
        plt.plot(data['x'],y_lasso,linewidth=4 , color='r')
        plt.plot(data['x'],data['y'],'.', color='b')
        plt.plot('Plot for alpha %.3g'%alpha)
        rss=sum((y_lasso - data['y'])**2)
        ret=[rss]
        ret.extend([lassoreg.intercept_])
        ret.extend(lassoreg.coef_)
        return ret
        ####################################
        # initialisation des prédicteurs
predictors=['x']
predictors.extend('x_%d'%i for i in range(2,16))
#définir les différents valeurs de alpha pour tester 
alpha_lasso=[1e-15, 1e-10, 1e-8, 1e-4, 1e-3,1e-2, 1, 5, 10, 20]
# initialisation des data frames pour stoker les coefficients 
col = ['rss','intercept'] + ['coef_x_%d'%i for i in range(1,16)]
ind = ['alpha_%.2g'%alpha_lasso[i] for i in range(0,10)]
coef_matrix_lasso = pd.DataFrame(index=ind, columns=col)

models_to_plot = {1e-15:231, 1e-10:232, 1e-4:233, 1e-3:234, 1e-2:235, 5:236}
for i in range(10):
    coef_matrix_lasso.iloc[i,] = lasso_regression(data, predictors, alpha_lasso[i], models_to_plot)
##########
coef_matrix_lasso.apply(lambda x: sum(x.values==0),axis=1)
# %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# ELASTIC NET 
